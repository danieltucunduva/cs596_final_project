{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS596 final project\n",
    "\n",
    "## Blind source separation with artificial neural networks\n",
    "\n",
    "#### Authors\n",
    "<pre style=\"font-size: 120%\">\n",
    "Daniel Correa Tucunduva  002228689\n",
    "Siqian Liu               002240226\n",
    "Weina Zhu                002242570\n",
    "Yan Jiang                002271130\n",
    "</pre>\n",
    "\n",
    "#### Credit\n",
    "<pre style=\"font-size: 120%\">\n",
    "Data generation, fastICA and results plot code adapted from:\n",
    "https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html\n",
    "\n",
    "Neural network code adapted from:\n",
    "https://raw.githubusercontent.com/abhijeet3922/Object-recognition-CIFAR-10/master/cifar10_90.py\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "N = 20000  # time points for synthetic data generation for training\n",
    "T = 10000  # time points for synthetic data generation for testing\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Utility library\n",
    "\n",
    "def random_weight():\n",
    "    # a random weight in the range [0.01, 1.00]\n",
    "    return round(np.random.randint(1, 101) / 100.0, 2)\n",
    "\n",
    "def learning_rate_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 5:\n",
    "        lrate = 0.0005\n",
    "    return lrate\n",
    "\n",
    "def save_generated_data_to_disk():\n",
    "    # weights and observations have to be flattened to 2d arrays to allow saving as .csv\n",
    "    flat_weights = np.zeros((GRID_ROWS * GRID_COLUMNS, 3))\n",
    "    row = 0\n",
    "    for i in range(GRID_ROWS):\n",
    "        for j in range(GRID_COLUMNS):\n",
    "            flat_weights[row] = weights[i][j]\n",
    "            row += 1\n",
    "    \n",
    "    flat_observations = np.zeros((N + T, GRID_ROWS * GRID_COLUMNS))\n",
    "    for nt in range(N + T):\n",
    "        flat_observations[nt] = np.ravel(observations[nt])\n",
    "    \n",
    "    np.savetxt('true_sources.csv', true_sources, delimiter=',', fmt='%1.2f')\n",
    "    np.savetxt('true_sources_noisy.csv', true_sources_noisy, delimiter=',', fmt='%1.2f')\n",
    "    np.savetxt('flat_weights.csv', flat_weights, delimiter=',', fmt='%1.2f')\n",
    "    np.savetxt('flat_observations.csv', flat_observations, delimiter=',', fmt='%1.2f')\n",
    "\n",
    "def save_cnn_to_disk():\n",
    "    cnn_json = cnn.to_json()\n",
    "    with open('cnn.json', 'w') as json_file:\n",
    "        json_file.write(cnn_json)\n",
    "    cnn.save_weights('cnn.h5')\n",
    "    \n",
    "def save_dense_network_to_disk():\n",
    "    dense_json = dense.to_json()\n",
    "    with open('dense.json', 'w') as json_file:\n",
    "        json_file.write(dense_json)\n",
    "    dense.save_weights('dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Module setup\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sources shape:  (30000, 3)\n",
      "Weights shape:  (10, 10, 3)\n",
      "Observations shape:  (30000, 10, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "GRID_ROWS = 10\n",
    "GRID_COLUMNS = 10\n",
    "\n",
    "time = np.linspace(0, (N + T) // 500, N + T)\n",
    "\n",
    "s1 = np.sin(2 * time) # sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time)) # square signal\n",
    "s3 = signal.sawtooth(2 * np.pi * time) # saw tooth signal\n",
    "\n",
    "# limit generated values to two decimals\n",
    "for nt in range(N + T):\n",
    "    s1[nt] = round(s1[nt], 2)\n",
    "    s2[nt] = round(s2[nt], 2)\n",
    "    s3[nt] = round(s3[nt], 2)\n",
    "\n",
    "# random non-zero weights\n",
    "weights = np.zeros((GRID_ROWS, GRID_COLUMNS, 3))\n",
    "for i in range(GRID_ROWS):\n",
    "    for j in range(GRID_COLUMNS):\n",
    "        weights[i][j][0] = random_weight()\n",
    "        weights[i][j][1] = random_weight()\n",
    "        weights[i][j][2] = random_weight()\n",
    "\n",
    "true_sources = np.c_[s1, s2, s3] # [N + T x 3], from signal tracks\n",
    "true_sources /= true_sources.std(axis=0) # standardize\n",
    "true_sources_noisy = true_sources + 0.15 * np.random.normal(size=true_sources.shape) # add noise\n",
    "\n",
    "# [N + T x 10 x 10], linear combinations of signals using random non-zero weights\n",
    "observations = np.zeros((N + T, GRID_ROWS, GRID_COLUMNS, 1))\n",
    "for nt in range(N + T):\n",
    "    for i in range(GRID_ROWS):\n",
    "        for j in range(GRID_COLUMNS):\n",
    "            linear_combination = weights[i][j][0] * true_sources_noisy[nt][0] +\\\n",
    "                                 weights[i][j][1] * true_sources_noisy[nt][1] +\\\n",
    "                                 weights[i][j][2] * true_sources_noisy[nt][2]\n",
    "            observations[nt][i][j][0] = round(linear_combination, 2)\n",
    "\n",
    "print('True sources shape: ', true_sources.shape)\n",
    "print('Weights shape: ', weights.shape)\n",
    "print('Observations shape: ', observations.shape)\n",
    "\n",
    "save_generated_data_to_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3)\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing\n",
    "\n",
    "true_sources_train = true_sources[:N]\n",
    "true_sources_test = true_sources[N:]\n",
    "\n",
    "true_sources_noisy_train = true_sources_noisy[:N]\n",
    "true_sources_noisy_test = true_sources_noisy[N:]\n",
    "\n",
    "observations_train = observations[:N]\n",
    "observations_test = observations[N:]\n",
    "\n",
    "print(true_sources_train.shape)\n",
    "print(true_sources_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 10, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10, 10, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 5, 5, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 5, 5, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 140,323\n",
      "Trainable params: 139,683\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN architecture\n",
    "\n",
    "weight_decay = 1e-4\n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=observations.shape[1:]))\n",
    "cnn.add(Activation('elu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "cnn.add(Activation('elu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "cnn.add(Activation('elu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "cnn.add(Activation('elu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.3))\n",
    "cnn.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "cnn.add(Activation('elu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "cnn.add(Activation('elu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.4))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(3, activation='linear'))\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "optimizer_cnn = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "cnn.compile(loss='mean_squared_error', optimizer=optimizer_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 14s 68ms/step - loss: 1.4665\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 0.2596\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.1206\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 0.1071\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 13s 63ms/step - loss: 0.1068\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 13s 67ms/step - loss: 0.1108\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 0.1099\n",
      "Epoch 8/10\n",
      " 73/200 [=========>....................] - ETA: 8s - loss: 0.1113"
     ]
    }
   ],
   "source": [
    "# CNN training\n",
    "\n",
    "generator = ImageDataGenerator()\n",
    "history_cnn = cnn.fit_generator(\n",
    "    generator.flow(observations_train, true_sources_noisy_train, batch_size=batch_size),\n",
    "    steps_per_epoch=observations_train.shape[0] // batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[LearningRateScheduler(learning_rate_schedule)]\n",
    ")\n",
    "\n",
    "save_cnn_to_disk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dense architecture\n",
    "\n",
    "dense = Sequential()\n",
    "dense.add(Dense(32, input_shape=observations.shape[1:]))\n",
    "dense.add(Dense(32))\n",
    "dense.add(Dense(64))\n",
    "dense.add(Dense(64))\n",
    "dense.add(Dense(128))\n",
    "dense.add(Dense(128))\n",
    "dense.add(Flatten())\n",
    "dense.add(Dense(3, activation='linear'))\n",
    "\n",
    "dense.summary()\n",
    "\n",
    "optimizer_dense = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "dense.compile(loss='mean_squared_error', optimizer=optimizer_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dense network training\n",
    "\n",
    "generator = ImageDataGenerator()\n",
    "history_dense = dense.fit_generator(\n",
    "    generator.flow(observations_train, true_sources_noisy_train, batch_size=batch_size),\n",
    "    steps_per_epoch=observations_train.shape[0] // batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[LearningRateScheduler(learning_rate_schedule)]\n",
    ")\n",
    "\n",
    "save_dense_network_to_disk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute CNN and Dense network predictions\n",
    "\n",
    "signals_cnn = cnn.predict(observations_test) \n",
    "signals_dense = dense.predict(observations_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA applied on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute ICA result\n",
    "\n",
    "# create [T x 100] track for ICA processing\n",
    "ica_track = np.zeros((T, 100))\n",
    "for t in range(T):\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            ica_track[t][i * 10 + j] = observations_test[t][i][j][0]\n",
    "\n",
    "\n",
    "ica = FastICA(n_components=3)\n",
    "unordered_signals_ica = ica.fit_transform(ica_track) # reconstruct signals\n",
    "unordered_signals_ica = np.array(unordered_signals_ica)\n",
    "unordered_signals_ica *= 100 # rescale signals\n",
    "\n",
    "# rearrange reconstructed signal tracks by smallest MSE against true source tracks\n",
    "signals_ica = np.zeros((T, 3))\n",
    "for track in range(3):\n",
    "    mse_sources_0 = mean_squared_error(true_sources_test[:,0], unordered_signals_ica[:,track])\n",
    "    mse_sources_1 = mean_squared_error(true_sources_test[:,1], unordered_signals_ica[:,track])\n",
    "    mse_sources_2 = mean_squared_error(true_sources_test[:,2], unordered_signals_ica[:,track])\n",
    "    if mse_sources_0 < mse_sources_1 and mse_sources_0 < mse_sources_2:\n",
    "        signals_ica[:,0] = unordered_signals_ica[:,track]\n",
    "    elif mse_sources_1 < mse_sources_0 and mse_sources_1 < mse_sources_2:\n",
    "        signals_ica[:,1] = unordered_signals_ica[:,track]\n",
    "    else:\n",
    "        signals_ica[:,2] = unordered_signals_ica[:,track]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compute mean squared error between results and original noise-free sources\n",
    "\n",
    "cnn_mean_squared_error = mean_squared_error(true_sources_test, signals_cnn)\n",
    "dense_mean_squared_error = mean_squared_error(true_sources_test, signals_dense)\n",
    "ica_mean_squared_error = mean_squared_error(true_sources_test, signals_ica)\n",
    "\n",
    "print('')\n",
    "print('Mean squared error of results against original noise-free sources:')\n",
    "print('')\n",
    "print('CNN           : {}'.format(cnn_mean_squared_error))\n",
    "print('')\n",
    "print('Dense network : {}'.format(dense_mean_squared_error))\n",
    "print('')\n",
    "print('ICA           : {}'.format(ica_mean_squared_error))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create plot of test data\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "models = [true_sources_test, true_sources_noisy_test, signals_cnn, signals_dense, signals_ica]\n",
    "names = ['Sources', 'Sources with noise', 'CNN results', 'Dense network results', 'ICA results']\n",
    "colors = ['red', 'steelblue', 'orange']\n",
    "\n",
    "for index, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.subplot(10, 1, index * 2)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
